# Private RAG Knowledge Base Chatbot

A fully local, private RAG (Retrieval-Augmented Generation) chatbot that keeps your data secure on your machine. No internet required for core functionality.

## Features

- **100% Local & Private**: All processing happens on your machine—no data leaves your device
- **Multiple File Types**: Supports PDF, DOCX, TXT, and PPTX documents
- **Smart Text Extraction**: Automatic extraction with fallback to UnstructuredPDFLoader for scanned PDFs
- **Beautiful UI**: Gradio 6.x interface with soft theme
- **Powerful Retrieval**: MMR search with k=8 results
- **Auto-Indexing**: Re-indexes on upload for simplicity (incremental indexing coming in future updates)
- **Clear Function**: Reset database and documents with one click
- **Chat History**: Maintains conversation context
- **Chunking**: 3000 tokens per chunk with 600-token overlap for better context

## Tech Stack (November 2025)

- **LLM**: Ollama + phi3:14b (8k context, excellent reasoning)
- **Embeddings**: nomic-embed-text (SOTA open embeddings)
- **Vector DB**: Chroma (persistent, local)
- **UI**: Gradio 6.x (fully local, no sharing)
- **Framework**: LangChain 0.3+
- **Document Loaders**: PyPDFLoader, Docx2txtLoader, TextLoader, UnstructuredPDFLoader (for scanned PDFs)

## Prerequisites

1. **Python 3.12+** (tested on macOS with venv)
2. **Ollama** installed and running locally with required models:
   ```bash
   ollama pull phi3:14b
   ollama pull nomic-embed-text
   ```

## Installation

1. Clone or download the project
2. Create and activate a virtual environment:
   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```
3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```
4. Run the application:
   ```bash
   python rag_bot.py
   ```

The app will launch at http://127.0.0.1:7863 (fully local, no tunneling).

## Usage

1. **Upload Documents**: Use the file upload area to add PDFs, DOCX, TXT, or PPTX files
2. **Index Documents**: Click "Index Documents" to process, chunk, and store embeddings
3. **Ask Questions**: Use the chat interface to query your documents (maintains chat history)
4. **Clear Data**: Click "Clear All" to reset the database and remove all documents

## How It Works

- Documents are loaded and chunked into 3000-token pieces with 600-token overlap
- Embeddings are generated locally using HuggingFace nomic-embed-text
- Chroma stores vectors persistently in `chroma_db/` folder
- Queries use MMR retrieval for diverse, relevant results (k=8)
- Answers are generated by phi3:14b using retrieved context only
- Chat history is preserved for follow-up questions

## Security & Privacy

- No data leaves your machine
- All processing is local
- Database stored in `chroma_db/` folder
- Documents stored in `documents/` folder
- No internet calls except to local Ollama instance

## Troubleshooting

- **NumPy warnings**: Harmless compatibility warnings—ignore
- **Import errors**: Ensure you're in venv and all packages from requirements.txt are installed
- **Ollama not found**: Verify Ollama is running: `ollama serve` and models are pulled
- **Port in use**: Edit `server_port=7863` in rag_bot.py if needed
- **Scanned PDFs not loading**: UnstructuredPDFLoader requires pdfminer.six and pillow-heif (included in requirements.txt)
- **Torch/Metal warnings**: macOS Metal acceleration warnings are harmless

## Future Roadmap

- Incremental indexing (no full re-index on upload)
- Hybrid search (BM25 + vector)
- Conversation memory persistence
- Source citations in answers
- Folder drag-and-drop support
- Export chat history

## License

This project is open-source. Modify and use as needed for personal/local use only.
